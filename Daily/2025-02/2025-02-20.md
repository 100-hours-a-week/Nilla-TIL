## 날짜: 2025-02-20

### 스크럼
- 잡, 인그레스,HPA

### 새로 배운 내용

#### (14) job과 cron job
    - job: 모든 컨테이너가 정상적으로 종료할 때까지 재실행 (파드 배치 처리..등등)
        - 프로그램의 실행 순서나 분기처리 등은 컨테이너 쉘 내에서 제어 해야 한다.
        - 여러 개의 파드가 존재 할 겨우 모든 컨테이너가 정상 종료 시까지 반복한다. (하나라도 비정상이면 전체 비정상) / 물론 실행 횟수를 종료하거나 재실행 횟수 도달 시 종료한다.
        - status가 complete여도 비정상 종료일지도 모른다!
        - 노드 장애 등 파드가 삭제된다면 다른 노드 파드에서 재실행한다.
    - cron job: unix의 크론과 같은 포맷으로, 실행 스케줄을 지정할 수 있는 컨트롤러
        - 파드 개수가 정해진 수를 넘으면 가비지 수집 컨트롤러가 종료 된 파드를 삭제한다.
    
    ```yaml
    #job-normal-end.yml
    
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: normal-end
    spec:
      template:
        spec:
          containers:
          - name: busybox
            image: busybox:latest
            command: ["sh", "-c", "sleep 5; exit 0"]
          restartPolicy: Never
      completions: 6
      #parallelism: 2  #패러럴리즘을 주석처리했다.                                                     
    ```
    
    ```bash
    **kubectl get jobs**
    NAME         STATUS     COMPLETIONS   DURATION   AGE
    normal-end   Complete   6/6           63s        7m15s
    
    **kubectl describe job**
    
    Annotations:      <none>
    Parallelism:      1
    Completions:      6
    
        Command:
          sh
          -c
          sleep 5; exit 
      Type    Reason            Age    From            Message
      ----    ------            ----   ----            -------
      Normal  SuccessfulCreate  7m31s  job-controller  Created pod: normal-end-wwhqx
      Normal  SuccessfulCreate  7m20s  job-controller  Created pod: normal-end-4v5zl
      Normal  SuccessfulCreate  7m9s   job-controller  Created pod: normal-end-t4tvp
      Normal  SuccessfulCreate  6m59s  job-controller  Created pod: normal-end-hlkz8
      Normal  SuccessfulCreate  6m49s  job-controller  Created pod: normal-end-7bf85
      Normal  SuccessfulCreate  6m38s  job-controller  Created pod: normal-end-kgnk8
      Normal  Completed         6m28s  job-controller  Job completed
      
    kubectl delete job normal-end
    
    #Parallelism 주석 처리를 풀었다 
    **kubectl apply -f job-normal-end.yml**
    
    **kubectl get jobs**
    NAME         STATUS     COMPLETIONS   DURATION   AGE
    normal-end   Complete   6/6           34s        66s
    
    kubectl describe job
    Annotations:      <none>
    Parallelism:      2
    Completions:      6
    Completion Mode:  NonIndexed
    Suspend:          false
    Backoff Limit:    6
    
        Command:
          sh
          -c
          sleep 5; exit 0
    
    Events:
      Type    Reason            Age   From            Message
      ----    ------            ----  ----            -------
      Normal  SuccessfulCreate  70s   job-controller  Created pod: normal-end-79v42
      Normal  SuccessfulCreate  70s   job-controller  Created pod: normal-end-7grzt
      Normal  SuccessfulCreate  59s   job-controller  Created pod: normal-end-fgd55
      Normal  SuccessfulCreate  58s   job-controller  Created pod: normal-end-zccqn
      Normal  SuccessfulCreate  48s   job-controller  Created pod: normal-end-5ggnb
      Normal  SuccessfulCreate  46s   job-controller  Created pod: normal-end-knf8d
      Normal  Completed         36s   job-controller  Job completed
    ```
    
- (14-1) 메시지 브로커와 job의 조합: **ec2 이슈 존재하는 듯**
    - 하나의 노드에서는 메모리 용량 제한이 있다.
    - 계산 범위를 분할, 여러 파드에서 병렬적으로 계산을수행하면 계산을 해결할 수 있다.
    - 계산 범위 파라미터를 메시지 브로커에 보내면 잡의 파드가 담당 범위를 계산
    
    rabbimq
    
    ```yaml
    **#taskQueue-deploy.yml**
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: taskqueue
    spec:
      selector:
        matchLabels:
          app: taskQueue
      replicas: 1
      template:
        metadata:
          labels:
            app: taskQueue   
        spec:
          containers:
          - image: rabbitmq
            name: rabbitmq
            ports:
            - containerPort: 5672
            resources:
              limits:
                cpu: 100m
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: taskqueue
    spec:
      type: NodePort
      ports:
      - port: 5672
        nodePort: 31672
      selector:
        app: taskQueue
    ```
    
    - **작업 큐 생성 (메시지 브로커)**:
        - **메시지 브로커**(예: **RabbitMQ**)는 작업을 수집하는 큐를 생성하고, **작업 요청을 큐에 넣음**
        - 예를 들어, 소수 계산 작업의 범위나 다른 처리할 작업들이 큐에 추가됨
    - **작업 큐에 메시지 삽입**:
        - 외부 시스템이나 다른 서비스에서 메시지 브로커로 작업 요청을 보내면, 메시지 브로커는 이를 큐에 저장
        - 예를 들어, `taskqueue`라는 큐에 작업 범위가 메시지로 들어감. (`[1,1000]`, `[1001,2000]` 등)
    - **Kubernetes Job 실행**:
        - **Kubernetes Job**은 이러한 큐에서 작업을 **병렬로 처리**. Kubernetes Job은 **여러 노드와 컨테이너를 사용**하여 병렬로 작업을 분배하고 실행.
        - **큐에 삽입된 작업**을 **`create_job_manifest`*에서 정의한 **Pod**로 처리. 각 **Job**은 지정된 **작업을 완료**할 때까지 실행.
    - **작업 완료 후 처리**:
        - 각 **Job**이 완료되면, **Pod**가 종료되고, 그 결과를 **메시지 브로커**가 기록할 수 있음. 또는, 다른 서비스가 결과를 받아서 후속 작업을 수행할 수도 있음
    
    ```python
    #job-initiator.py
    #rabbitmq를 사용해 작업을 병렬로 처리하기 위해 job리소스를 생성하는 파이썬 스크립트
    #큐에 작업을 추가하고, 그 작업을 쿠버네티스에 배포해 병렬로 실행함
    
    #!/usr/bin/env python
    # -*- coding:utf-8 -*-
    
    #from os import path
    import yaml
    import pika
    from kubernetes import client, config
    
    OBJECT_NAME = "pngen"
    qname = 'taskqueue'
    
    # 메시지 브로커 접속
    def create_queue():
        qmgr_cred= pika.PlainCredentials('guest', 'guest')
        #qmgr_host='172.16.20.11'  # for vagrant-k8s
        #qmgr_host='192.168.99.100' # for minikube
        qmgr_host='192.168.45.42' # Windows host IP
        qmgr_port='31672'
        qmgr_pram = pika.ConnectionParameters(
        	      host=qmgr_host,
    	      port=qmgr_port,
    	      credentials=qmgr_cred)
        conn = pika.BlockingConnection(qmgr_pram)
        chnl = conn.channel()
        chnl.queue_declare(queue=qname)
        return chnl
    
    # 잡 매니페스트 작성
    def create_job_manifest(n_job, n_node):
        container = client.V1Container(
            name="pn-generator",
            image="maho/pn_generator:0.7",
            env=[
                client.V1EnvVar(name="BROKER_URL",value="amqp://guest:guest@taskqueue:5672"),
                client.V1EnvVar(name="QUEUE",value="taskqueue")
            ]
        )
        template = client.V1PodTemplateSpec(
            spec=client.V1PodSpec(containers=[container],
                                  restart_policy="Never"                              
            ))
        spec = client.V1JobSpec(
            backoff_limit=4,
            template=template,
            completions=n_job,
            parallelism=n_node)
        job = client.V1Job(
            api_version="batch/v1",
            kind="Job",
            metadata=client.V1ObjectMeta(name=OBJECT_NAME),
            spec=spec)
        return job
    
    if __name__ == '__main__':
    
        # 소수 계산 분할 파라미터
        job_parms = [[1,1000],[1001,2000],[2001,2000],[3001,4000]]
        jobs  = len(job_parms)
        nodes = 2
    
        # 큐에 삽입
        queue = create_queue()
        for param_n in job_parms:
            param = str(param_n).replace('[','').replace(']','')
            queue.basic_publish(exchange='',routing_key=qname,body=param)
    
        # kubectl의.kube config를 읽어 k8s마스터에 잡 요청 전송
        config.load_kube_config()
        client.BatchV1Api().create_namespaced_job(
            body=create_job_manifest(jobs,nodes),namespace="default")
    
    ```
    
    ```bash
    #메시지 브로커와 조합한 이미지를 빌드한다.
    
    docker build --tag pn_generator:0.2 .
    docker tag pn_generator:0.2 mushr00mandu/pn_generator:0.2
    docker push mushr00mandu/pn_generator:0.2
    cd ..
    
    **kubectl apply -f taskQueue-deploy.yml**
    deployment.apps/taskqueue created
    service/taskqueue created
    
    **kubectl get pod,svc**
    NAME                             READY   STATUS             RESTARTS        AGE
    pod/taskqueue-68dcc9dc5f-nd9mx   1/1     Running            0               98s
    
    NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
    service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          3d2h
    service/taskqueue    NodePort    10.102.21.99   <none>        5672:31672/TCP   98s
    
    docker build --tag job-init:0.1 .
    
    #그러나 실패함
    #왜일까: 아마 위에 로드밸런서 실패한 이유(밖에서 접근안됨) 이슈인듯
    **job-initiator$ docker run -it --rm --name kube -v "$(pwd)"/py:/py -v ~/.kube:/root/.kube job-init:0.1 bash**
    root@5b3163dcc880:/# cd py
    root@5b3163dcc880:/py# python3 job-initiator.py
    Traceback (most recent call last):
      File "job-initiator.py", line 63, in <module>
        queue = create_queue()
      File "job-initiator.py", line 23, in create_queue
        conn = pika.BlockingConnection(qmgr_pram)
      File "/usr/local/lib/python3.8/dist-packages/pika/adapters/blocking_connection.py", line 360, in __init__
        self._impl = self._create_connection(parameters, _impl_class)
      File "/usr/local/lib/python3.8/dist-packages/pika/adapters/blocking_connection.py", line 451, in _create_connection
        raise self._reap_last_connection_workflow_error(error)
    pika.exceptions.AMQPConnectionError
    
    ```
    
#### (15) 인그레스
    - 인그레스: 쿠버네티스에서 **HTTP(S) 트래픽을 클러스터 내로 라우팅하는 리소스**
        - **로드밸런서**와 웹 서버 (nginx, HAProxy 등)를 사용하여 외부 요청을 받아 **쿠버네티스 클러스터 내의 서비스**로 전달
        - 서비스와 연결하여 애플리케이션을 공개하고, ssl/tsl 암호화나 세션 어피니티 기능을 갖추고 있다.
        - 기능 : 공개 url과 애플리케이션 매핑 / 복수 도메인 이름을 가지는 가상 호스트 / 클라이언트의 요청을 여러 파드에 분산 / ssl/tsl 암호화 통신 https 프로토콜 지원 / 세션 어피니티
        - 인그레스로 **로드밸런서 대체** 가능하다.
    - 인그레스 환경 설정
    
    ```bash
    #환경설정
    minikube addons enable ingress
    minikube addons list
    ingress                     | minikube | enabled ✅   | Kubernetes        
    ```
    
    - 인그레스 매니페스트에서는 메타데이터와 어노테이션이 중요한 역할을 수행한다
    - 어노테이션에서는 키, 값을 기재해서 인그레스 컨트롤러에 명령을 전달한다.
    - [kubernetes.io/ingress.class:’nginx’](http://kubernetes.io/ingress.class:’nginx’) 여러 인그레스 컨트롤러가 클러스터에 동작중이라면 어노테이션을 명시적으로 지정해야함
    - nginx.ingress.Kubernetes.io/rewrite-target: /URL 경로를 바꾸도록 지정하는 어노테이션
        - 이 설정이 없다면 클라이언트 요청을 그대로 전송해 file notfound 에러 발생!
    - 컨피그맵(애플리케이션의 설정 값을 저장하는 쿠버네티스 리소스. 환경 변수, 설정 파일 등을 저장)
        - 미들웨어(nginx, apache)의 설정 파일을 컨피그 맵으로 설정하면 파드는 디스크처럼 마운트하여 네임스페이스에서 설정값을 읽어들일 수 있다.(동적으로 읽어들일 수 있다.)
        - 설정 파일을 **동적으로 수정**하면, 파드를 **재배포하거나 다시 시작하지 않고**도 설정을 **바로 적용**할 수 있다는 장점이 존재.
    - 인그레스 적용
    
    ```yaml
    #hello-world.yml
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:ㄹ
      name: hello-world-deployment
    spec:
      replicas: 5
      template:
        metadata:
          labels:
            app: hello-world
        spec:
          containers:
            - image: "strm/helloworld-http"
              imagePullPolicy: Always
              name: hello-world-container
              ports:
                - containerPort: 8080
      selector:
        matchLabels:
          app: hello-world
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: hello-world-svc
      labels:
        app: hello-world-svc
    spec:
      type: NodePort
      ports:
         -  port: 8080
            protocol: TCP
            targetPort: 80
            nodePort: 31445
      selector:
        app: hello-world
    
    #ingress.yml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: hello-world-ingress
    spec:
      rules:
      - host: example.com
        http:
          paths:
          - path: / #/로 요청이 오면 받는다는 뜻임
            pathType: Prefix
            backend:
              service:
                name: hello-world-svc
                port:
                  number 8080
    
    ```
    
    ```bash
    **#제대로 등록이 되었는지 확인
    kubectl get svc,deploy**
    NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
    service/hello-world-svc   NodePort    10.99.152.187   <none>        8080:31445/TCP   34s
    service/kubernetes        ClusterIP   10.96.0.1       <none>        443/TCP          3d3h
    service/taskqueue         NodePort    10.102.21.99    <none>        5672:31672/TCP   74m
    
    NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
    deployment.apps/hello-world-deployment   5/5     5            5           34s
    deployment.apps/taskqueue                1/1     1            1           74m
    
    **kubectl get ingress**
    NAME                  CLASS   HOSTS         ADDRESS        PORTS   AGE
    hello-world-ingress   nginx   example.com   192.168.49.2   80      2m47s
    
    #이후 호스트 ip(192.168.49.2)에 도메인을 두개 등록한다 ( abc.sample.com, xyz.sample.com)
    **sudo vi /etc/hosts
    
    #curl 요청을 해보면 잘 연결된다.**
    **curl abc.sample.com**
    <html>
    <head><title>404 Not Found</title></head>
    <body>
    <center><h1>404 Not Found</h1></center>
    <hr><center>nginx</center>
    </body>
    </html>
    **curl xyz.sample.com**
    <html>
    <head><title>404 Not Found</title></head>
    <body>
    <center><h1>404 Not Found</h1></center>
    <hr><center>nginx</center>
    </body>
    </html>
    
    ```
    
    - TLS 암호화를 포함한 인그레스 매니페스트
    
    ```bash
    ingress-tls.yml
    
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: secure-ingress
      annotations:
        nginx.ingress.kubernetes.io/ssl-redirect: "true"  # HTTP 요청을 HTTPS로 리디렉션
    spec:
      tls:
      - hosts:
        - example.com  # 이 도메인에 대해 TLS를 활성화
        secretName: tsl-certificate  # 생성한 TLS 인증서를 사용
      rules:
      - host: example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service  # 트래픽을 라우팅할 서비스 이름
                port:
                  number: 80
    
    ```
    
    ```bash
    #인증서 발급 받기
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout nginx-selfsigned.key -out nginx-selfsigned.crt
    
    #tls secret 리소스를 생성
    **kubectl create secret tls tsl-certificate --key nginx-selfsigned.key --cert nginx-selfsigned.crt
    secret/tsl-certificate created**
    
    kubectl get secret
    NAME              TYPE                DATA   AGE
    tsl-certificate   kubernetes.io/tls   2      12s
    
    kubectl apply -f ingress-tls.yml
    
    #직접 쓴 인증서는 못믿겠다고 뭐라뭐라한다
    curl https://example.com
    curl: (60) SSL certificate problem: self-signed certificate
    More details here: https://curl.se/docs/sslcerts.html
    
    curl failed to verify the legitimacy of the server and therefore could not
    establish a secure connection to it. To learn more about this situation and
    how to fix it, please visit the web page mentioned above.
    
    #--insecure 옵션 주면 잘 접속 된다. (오류는 따로 서비스를 안올렸기 때문)
    curl https://example.com --insecure
    <html>
    <head><title>503 Service Temporarily Unavailable</title></head>
    <body>
    <center><h1>503 Service Temporarily Unavailable</h1></center>
    <hr><center>nginx</center>
    </body>
    </html>
    ```
    
    - session affinity 기능
    - 모더니제이션
        - 웹에서는 로드밸런서의 세션 어피니티를 이용한다
        - 브라우저는 무상태 프로토콜로, 서버 클라이언트 통신을 유지할 수 없다
        - 일반적으로는 웹은 브라우저를 식별하기 위한 쿠키로 세션 정보를 얻어서 요청을 처리한다
        - 로드 밸런서가 앞단에 있다면 세션 정보를 가지지 않은 서버에 보낼지도 모르는데, 이를 위해 세션 어피니티 사용!
        - 인그레스 세션 어피니티를 사용해서 최소한으로 변경하고, 애플리케이션 가동 중 롤아웃 불가하다.
    
    ```yaml
    #ingress-session.yml에 버전 문제가 있어 형식 고침
    
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: hello-ingress
      annotations:
        kubernetes.io/ingress.class: 'nginx'
        nginx.ingress.kubernetes.io/affinity: 'cookie'
    spec:
      rules:
      - host: abc.sample.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: session-svc
                port:
                  number: 9080                                  
    ```
    
    ```yaml
    #session-test.yml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: session-deployment
    spec:
      selector:
        matchLabels:
          app: session
      replicas: 10
      template:
        metadata:
          labels:
            app: session
        spec:
          containers:
            - image: 'mushr00mandu/session-test' #여기 안바꾸면 이상한 이미지 다운받게됨
            #보안상 위험하니 조심하자...
              name: session
              ports:
                - containerPort: 80
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: session-svc
    spec:
      selector:
        app: session
      ports:
        - port: 9080
          targetPort: 80
    ```
    
    ```docker
    FROM php:7.0-apache
    COPY php/ /var/www/html/
    RUN chmod a+rx /var/www/html/*.php
    ```
    
    ```bash
    docker build --tag mushr00mandu/session-test .
    docker login
    docker push mushr00mandu/session-test
    kubectl apply -f session-test.yml
    kubectl apply -f ingress-session.yml
    
    #쿠키를 저장해!
    curl -c cookie.dat http://abc.sample.com/
    Hostname: session-deployment-64446b586d-xrs64<br>
    1th time access.
    
    #저장한 쿠키 바탕으로 계속 연결해!
    curl -b cookie.dat http://abc.sample.com/
    Hostname: session-deployment-64446b586d-xrs64<br>
    2th time access.
    curl -b cookie.dat http://abc.sample.com/
    Hostname: session-deployment-64446b586d-xrs64<br>
    3th time access.
    curl -b cookie.dat http://abc.sample.com/
    Hostname: session-deployment-64446b586d-xrs64<br>
    4th time access.
    curl -b cookie.dat http://abc.sample.com/
    Hostname: session-deployment-64446b586d-xrs64<br>
    5th time access.
    ```
    
    - 고가용성 인그레스 환경 구축
        - nginx 인그레스는 노드간 공유 기능이 없음
        - kube-keepalived-vip 파드는 VRRP(네트워크 이중화)에 따라 노드에 VIP를 할당하고 인그레스로 전달함
    
    ```bash
    ls ..
    README.md  ingress-keepalived  ingress-keepalived.zip  session_affinity  test-apl  test-apl.zip  url-mapping
    
    **kubectl apply -f ingress-keepalived/**
    namespace/tkr-system created
    configmap/nginx-configuration created
    configmap/tcp-services created
    configmap/udp-services created
    deployment.apps/nginx-ingress-controller created
    deployment.apps/default-http-backend created
    service/default-http-backend created
    serviceaccount/nginx-ingress-serviceaccount created
    clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
    clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-binding created
    configmap/vip-configmap created
    daemonset.apps/kube-keepalived-vip created
    serviceaccount/kube-keepalived-vip created
    clusterrole.rbac.authorization.k8s.io/kube-keepalived-vip created
    clusterrolebinding.rbac.authorization.k8s.io/kube-keepalived-vip created
    
    **kubectl get ds,svc,po -n tkr-system**
    NAME                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
    daemonset.apps/kube-keepalived-vip   1         1         1       1            1           <none>          27s
    
    NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
    service/default-http-backend   ClusterIP   10.106.5.159   <none>        80/TCP    27s
    
    NAME                                            READY   STATUS              RESTARTS   AGE
    pod/default-http-backend-b5c8d79cf-xv87z        1/1     Running             0          27s
    pod/kube-keepalived-vip-2wgmn                   1/1     Running             0          26s
    pod/nginx-ingress-controller-5b49667449-8hh9c   0/1     ContainerCreating   0          27s
    pod/nginx-ingress-controller-5b49667449-nvg68   0/1     ContainerCreating   0          27s
    
    kubectl apply -f ingress.yml
    ingress.networking.k8s.io/hello-world-ingress created
    kubectl apply -f hello-world.yml
    deployment.apps/hello-world-deployment unchanged
    service/hello-world-svc unchanged
    
    **curl http://example.com**
    <!doctype html>
    <html>
    <head>
        <title>Example Domain</title>
    
    ...#생략
    
    #여기는 따로 라우팅이 안되어서 이렇게 나온다.
    curl --header "Host: abc.sample.com" http://192.168.49.2/
    <html>
    <head><title>404 Not Found</title></head>
    <body>
    <center><h1>404 Not Found</h1></center>
    <hr><center>nginx</center>
    </body>
    </html>
    
    ```
    
#### (15-1) HPA 설정과 부하 테스트 : **부하테스트가 제대로 이루어지지 않음**
- HPA 설정과 부하 테스트
    - 부하에 반응하여 자동으로 처리 능력을 조절한다
    - cpu와 메모리 사용률에 따라 노드의 수를 자동으로 조정한다.
    
    ```bash
    **#초기 설정(metric 서버 키기)
    minikube addons list | grep metric**
    | metrics-server              | minikube | disabled     | Kubernetes                     |
    
    **minikube addons enable metrics-server**
    
    **kubectl get deployment metrics-server -n kube-system**
    NAME             READY   UP-TO-DATE   AVAILABLE   AGE
    metrics-server   0/1     1            0           46s
    
    **#pod들의 cpu 사용량을 볼 수 있다
    kubectl top pod**
    NAME                                      CPU(cores)   MEMORY(bytes)   
    hello-world-deployment-685b7fc4f6-6dhln   1m           6Mi             
    hello-world-deployment-685b7fc4f6-88x2g   1m           6Mi             
    hello-world-deployment-685b7fc4f6-9mz6q   1m           6Mi             
    hello-world-deployment-685b7fc4f6-m9ck4   1m           6Mi             
    hello-world-deployment-685b7fc4f6-mdh7g   1m           13Mi            
    load-bal                                  0m           9Mi             
    taskqueue-68dcc9dc5f-nd9mx                4m           91Mi     
    
    ```
    
    ```docker
    docker build --tag mushr00mandu/web-php:0.2 .
                                                                                                 0.0s
    docker push mushr00mandu/web-php:0.2
    
    kubectl apply -f autoscale.yml 
    deployment.apps/web-php created
    service/web-php created
    
    **kubectl autoscale deployment web-php --cpu-percent=50 --min=1 --max=10**
    horizontalpodautoscaler.autoscaling/web-php autoscaled
    
    **kubectl get deploy,rs,hpa,po,svc**
    NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
    deployment.apps/web-php                  1/1     1            1           66s
    
    NAME                                                DESIRED   CURRENT   READY   AGE       169m
    replicaset.apps/web-php-f7b9685b4                   1         1         1       66s
    
    NAME                                          REFERENCE            TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
    horizontalpodautoscaler.autoscaling/web-php   Deployment/web-php   cpu: <unknown>/50%   1         10        0          16s
    
    NAME                                          READY   STATUS    RESTARTS        AGE           169m
    pod/web-php-f7b9685b4-pl8vj                   1/1     Running   0               66s
    
    NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
    service/web-php           NodePort    10.103.13.195   <none>        80:31446/TCP     66s
    
    kubectl run -it busybox --restart=Never --rm --image=busybox sh
    
    / # while true; do wget -q -O - http://web-php:80 > /dev/null; done
    
    wget: can't connect to remote host (10.103.13.195): Connection refused
    wget: can't connect to remote host (10.103.13.195): Connection refused
    
    #서버를 죽여버렸다
    
    while true; do kubectl get hpa; sleep 10; done #이 명령어는 kubectl에서 쳐야한다.
    
    ```
    
    
 <img src="img/66.png">
    
    아무래도 hpa가 제대로 늘려줬긴 했나보다. 리소스 부족 현상을 몸소 체험했다.
    
    ```docker
    sudo systemctl restart docker 
    #당황하지 말고 도커 재시작하고 minikube start하자
    
    kubectl run -it busybox --restart=Never --rm --image=busybox sh
    The connection to the server 192.168.49.2:8443 was refused - did you specify the right host or port?
    minikube status
    minikube
    type: Control Plane
    host: Running
    kubelet: Running
    apiserver: Running
    kubeconfig: Configured
    
    #이유는 모르겠지만 이렇게 하라고함...
    #클러스터 설정을 kubectl에게 전달하는 역할인듯
    eval $(minikube -p minikube docker-env)
    
    #이후 되살아났다!
    
    ```

    
그런데 다시 해봤더니 딱히 cpu 사용량이 늘지는 않았다. 다른 터미널로 한 것이 영향이 있는 걸까? 
    
#### (etc) 번외: ssh로 ec2 서버에 파일 옮기기
    
    다음의 순서대로 진행하면 된다: 
    
    `chmod 600 (keypair-kube-master.ppk파일 경로)` 
    
    `puttygen (keypair-kube-master.ppk파일 경로)  -O private-openssh -o (keypair-kube-master.pem 파일 경로)` 
    
    `scp -i (keypair-kube-master.pem 파일 경로) -r (복사 파일 경로) ubuntu@(나의 ip):/home/ubuntu/`
    
    시행착오와 설명: 
    
    ```python
    #첫 시도 :  SSH private key의 파일 권한이 너무 열려 있어서 발생한 문제
    #0644 권한은 너무 열려 있기 때문에, 보안상 읽기 전용 권한으로 제한해야 함
    scp -i (keypair-kube-master.ppk파일 경로) (복사 파일 경로) ubuntu@(나의 ip):/home/ubuntu/
    
    The authenticity of host (나의 ip) can't be established.
    Warning: Permanently added (나의 ip) (ED25519) to the list of known hosts.
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    @         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
    @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    Permissions 0644 for are too open.
    It is required that your private key files are NOT accessible by others.
    This private key will be ignored.
    Load key bad permissions
    
    #그래서 권한을 주고 다시 했다.
    **chmod 600 (keypair-kube-master.ppk파일 경로)** 
    
    #두번째 시도: PPK 형식의 SSH 키 파일을 OpenSSH 형식으로 변환하지 않고 사용하려고 할 때 발생하는 문제
    scp -i (keypair-kube-master.ppk파일 경로) (복사 파일 경로) ubuntu@(나의 ip):/home/ubuntu/
    Load key (keypair-kube-master.ppk파일 경로) : invalid format
    
    #putty를 설치하고 형식을 바꿔서 다시 시도
    **puttygen (keypair-kube-master.ppk파일 경로)  -O private-openssh -o (keypair-kube-master.pem 파일 경로)** 
    
    #또한 나는 폴더를 올렸기 때문에 -r 옵션이 필요
    **scp -i (keypair-kube-master.pem 파일 경로) -r (복사 파일 경로) ubuntu@(나의 ip):/home/ubuntu/**
    
    ingress.yml                                   100%  301    11.4KB/s   00:00    
    hello-world.yml                               100%  662    58.7KB/s   00:00    
    test-apl.zip                                  100%  731    69.4KB/s   00:00    
    ing-rbac.yml                                  100% 1454    86.0KB/s   00:00    
    a-namespace.yml                               100%   61     5.9KB/s   00:00    
    ... #생략    
    
    #정상 복사 완료!
    ```

    ### 오늘의 도전 과제와 해결 방법
    - EC2 환경설정에서 많은 시행착오를 겪었다. 중간에 서버가 꺼져 제대로 동작하지 않는 쿠버네티스를 되살리는 작업도 했는데, 위의 15번 항목에 서술하였다.
    
    ### 오늘의 회고
    - 실습 후에 인스턴스를 안끄고 자는 실수가 있었다. 금액 보고 정신이 들어서 반드시 꺼야겠다.ㅠㅠ
    
    ### 참고 자료 및 링크
    - x
    